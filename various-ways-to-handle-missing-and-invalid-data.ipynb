{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.358483Z",
     "iopub.status.busy": "2022-02-11T18:36:16.358226Z",
     "iopub.status.idle": "2022-02-11T18:36:16.366058Z",
     "shell.execute_reply": "2022-02-11T18:36:16.365257Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.358456Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Handle the null values in the dataset is one of the important steps in data wrangling. These null values adversely affect the performance and accuracy of any machine learning algorithm. So, it is very important to handle null values in the dataset before applying any machine learning algorithm to that dataset. Although some algorithms like XGBoost have built-in feature to handle null values, but we should also do it manually as a good practice while preparing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Null Values from Dataset in Python using Pandas Library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.385105Z",
     "iopub.status.busy": "2022-02-11T18:36:16.384823Z",
     "iopub.status.idle": "2022-02-11T18:36:16.405963Z",
     "shell.execute_reply": "2022-02-11T18:36:16.405297Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.385074Z"
    }
   },
   "outputs": [],
   "source": [
    "# We will use Titanic dataset\n",
    "dataset = pd.read_csv('../input/titanicdataset-traincsv/train.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.407679Z",
     "iopub.status.busy": "2022-02-11T18:36:16.407286Z",
     "iopub.status.idle": "2022-02-11T18:36:16.42094Z",
     "shell.execute_reply": "2022-02-11T18:36:16.420012Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.407623Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.422628Z",
     "iopub.status.busy": "2022-02-11T18:36:16.42231Z",
     "iopub.status.idle": "2022-02-11T18:36:16.453714Z",
     "shell.execute_reply": "2022-02-11T18:36:16.453047Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.4226Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will just remove null values from our Titanic dataset as a part of data wrangling step in order to make our article short and crisp.\n",
    "\n",
    "Lets see how many null values are there in our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.455199Z",
     "iopub.status.busy": "2022-02-11T18:36:16.454992Z",
     "iopub.status.idle": "2022-02-11T18:36:16.463708Z",
     "shell.execute_reply": "2022-02-11T18:36:16.462833Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.455173Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get a list of features with the count of null values. From the output of the above code, it is clear that Age column contains 177 null values and Cabin column contains 687 null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets drop the Cabin column\n",
    "\n",
    "We see that Cabin column contains 687 null values out of 891 rows / observations. So, it makes sense to drop this column from the dataset. Lets drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.466274Z",
     "iopub.status.busy": "2022-02-11T18:36:16.465694Z",
     "iopub.status.idle": "2022-02-11T18:36:16.473602Z",
     "shell.execute_reply": "2022-02-11T18:36:16.472949Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.46622Z"
    }
   },
   "outputs": [],
   "source": [
    "# Please note that inplace parameter is used to permanently affect our dataset. \n",
    "# By default, it is false. If we don't set it to True explicitly, the Cabin column is not dropped permanently from our dataset.\n",
    "dataset.drop('Cabin', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.475003Z",
     "iopub.status.busy": "2022-02-11T18:36:16.474661Z",
     "iopub.status.idle": "2022-02-11T18:36:16.484552Z",
     "shell.execute_reply": "2022-02-11T18:36:16.483698Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.474975Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets drop all the rows in the dataset which contain null values\n",
    "# dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will remove all the rows which contain null values from the dataset. Now our dataset does not contain any null value. This step is not recommended. I added this step just for illustration. We can loose significant information by executing this step. There are methods to replace the null values with some meaningful values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Imputer\n",
    "Imputer class present in Scikit Learn library is used to replace the missing values in the numeric feature with some meaningful value like mean, median or mode. Lets see its implementation in Python using sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.486932Z",
     "iopub.status.busy": "2022-02-11T18:36:16.486553Z",
     "iopub.status.idle": "2022-02-11T18:36:16.49537Z",
     "shell.execute_reply": "2022-02-11T18:36:16.494489Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.4869Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the required libraries like pandas, numpy and sklearn\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.497176Z",
     "iopub.status.busy": "2022-02-11T18:36:16.496411Z",
     "iopub.status.idle": "2022-02-11T18:36:16.5147Z",
     "shell.execute_reply": "2022-02-11T18:36:16.513995Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.497129Z"
    }
   },
   "outputs": [],
   "source": [
    "# I have added two features (added 10 values in each feature) in this data frame and deliberately put two nan values in the second feature. \n",
    "# We will impute these nan values using Imputer class present in sklearn library.\n",
    "\n",
    "dataframe = pd.DataFrame()\n",
    "dataframe['Feature_1'] = [0.42,0.56,0.36,0.90,0.98,0.64,0.76,0.56,0.39,0.77]\n",
    "dataframe['Feature_2'] = [np.nan,0.90,0.75,0.45,np.nan,0.88,0.67,0.34,0.72,0.28]\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of Imputer class parameters:\n",
    "\n",
    "missing_values — This is the value which has to be replaced in the dataset. This could either be an integer, or NaN. If you don’t pass this value, NaN will be the default value. So, wherever we have NaN in our dataset, the Imputer object will replace it with a new value.\n",
    "\n",
    "strategy — This is the strategy we’ll be using to calculate the value which has to replace the NaN occurrences in the dataset. There are three different strategies we can use: mean, median, most_frequent.\n",
    "\n",
    "verbose — This will just decide the verbosity of the Imputer. By default, it’s set to 0.\n",
    "\n",
    "copy — This will decide if a copy of the original object has to be made, or if the Imputer should change the dataset in-place. By default, it is set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.611898Z",
     "iopub.status.busy": "2022-02-11T18:36:16.611203Z",
     "iopub.status.idle": "2022-02-11T18:36:16.620782Z",
     "shell.execute_reply": "2022-02-11T18:36:16.619967Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.611861Z"
    }
   },
   "outputs": [],
   "source": [
    "# Impute nan values with mean value using Imputer class\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean',verbose=0,copy=\"True\")\n",
    "dataframe = imputer.fit_transform(dataframe.values)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace missing values\n",
    "We will use the Titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.623126Z",
     "iopub.status.busy": "2022-02-11T18:36:16.622673Z",
     "iopub.status.idle": "2022-02-11T18:36:16.633603Z",
     "shell.execute_reply": "2022-02-11T18:36:16.632795Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.623085Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets see how many missing values are there in the \"Age\" column of Titanic dataset\n",
    "dataset['Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 177 missing values out of 891 observations. Now how to handle these 177 missing values? \n",
    "\n",
    "The general method to handle such kind of scenarios is to replace the missing values with some meaningful value. This meaningful values can be obtained by taking the mean, median or mode of all the not null values in the \"Age\" column. This is a statistical approach of handling the missing values and is well suited for linear data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.635548Z",
     "iopub.status.busy": "2022-02-11T18:36:16.635094Z",
     "iopub.status.idle": "2022-02-11T18:36:16.649854Z",
     "shell.execute_reply": "2022-02-11T18:36:16.648939Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.635504Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate mean, median and mode \n",
    "\n",
    "mean_age = dataset['Age'].mean()\n",
    "median_age = dataset['Age'].median()\n",
    "mode_age = dataset['Age'].mode()\n",
    "display('Mean Age: ' + str(mean_age))\n",
    "display('Median Age: ' + str(median_age))\n",
    "display('Mode Age: ' + str(mode_age))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the missing values in the \"Age\" column with any of the above calculated values. In this case, I am going to replace the missing values with the mean value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.651267Z",
     "iopub.status.busy": "2022-02-11T18:36:16.65096Z",
     "iopub.status.idle": "2022-02-11T18:36:16.658471Z",
     "shell.execute_reply": "2022-02-11T18:36:16.657706Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.651239Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset['Age'].replace(np.NaN, mean_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that, this is just an approximation of the missing values and it may lead to variance in the prediction but we have to deal with it. There is no way around. But this approach is far better than dropping the \"Age\" column due to which we will lose a lot of significant data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace missing value in Categorical Column\n",
    "\n",
    "We had seen how to impute missing values in numeric columns, Now will see how can we impute missing values in categorical columns\n",
    "\n",
    "Again, we will take example of Titanic dataset. There are two categorical columns (Cabin and Embarked) in the Titanic dataset which have missing values. Cabin has 687 missing values (out of 891), so its better to drop this column as it has more than 77% of null data. So, lets concentrate on Embarked column which has only 2 missing values.\n",
    "\n",
    "In categorical columns, we introduce a new category usually called \"Unknown\" to impute missing values. As this column has 'S', 'C' and 'Q' categories, lets impute 'U' (Unknown) as a new category for 2 missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.661714Z",
     "iopub.status.busy": "2022-02-11T18:36:16.66121Z",
     "iopub.status.idle": "2022-02-11T18:36:16.670613Z",
     "shell.execute_reply": "2022-02-11T18:36:16.669703Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.661677Z"
    }
   },
   "outputs": [],
   "source": [
    "# We want to replace those nan values\n",
    "dataset['Embarked'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.67271Z",
     "iopub.status.busy": "2022-02-11T18:36:16.672256Z",
     "iopub.status.idle": "2022-02-11T18:36:16.679289Z",
     "shell.execute_reply": "2022-02-11T18:36:16.678704Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.672669Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset['Embarked'].fillna('U',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.680837Z",
     "iopub.status.busy": "2022-02-11T18:36:16.680442Z",
     "iopub.status.idle": "2022-02-11T18:36:16.690577Z",
     "shell.execute_reply": "2022-02-11T18:36:16.689961Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.680786Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset['Embarked'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Categorical Variables into Dummies\n",
    "Categorical variables are those variables which contain categorical values. For example, consider the \"Sex\" column in our Titanic dataset. It is categorical variable containing male and female. We need to convert these categories (male and female) into numbers (0 and 1) because most of the machine learning algorithms don't accept string values.\n",
    "\n",
    "I found 3 categorical columns (Sex, Embarked and Pclass) in the Titanic dataset. So, lets convert them into numbers (0 and 1). Or, we can say lets one hot encode these variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas library in Python contains get_dummies method which does the one hot encoding of the categorical variables (converts them into numbers - 0 and 1). The method get_dummies creates a new data frame which consists of zeros and ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.692169Z",
     "iopub.status.busy": "2022-02-11T18:36:16.691778Z",
     "iopub.status.idle": "2022-02-11T18:36:16.702592Z",
     "shell.execute_reply": "2022-02-11T18:36:16.701934Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.692128Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert categorical variables to their respective one hot encoded representation\n",
    "\n",
    "sex = pd.get_dummies(dataset['Sex'],prefix=\"Sex\")\n",
    "embark = pd.get_dummies(dataset['Embarked'],prefix=\"Embarke\")\n",
    "pclass = pd.get_dummies(dataset['Pclass'],prefix=\"pclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.704406Z",
     "iopub.status.busy": "2022-02-11T18:36:16.703996Z",
     "iopub.status.idle": "2022-02-11T18:36:16.716445Z",
     "shell.execute_reply": "2022-02-11T18:36:16.715743Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.704364Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sex.head())\n",
    "print(embark.head())\n",
    "print(pclass.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.718488Z",
     "iopub.status.busy": "2022-02-11T18:36:16.718035Z",
     "iopub.status.idle": "2022-02-11T18:36:16.727192Z",
     "shell.execute_reply": "2022-02-11T18:36:16.726343Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.718441Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate all the one hot encoded columns to the original dataset\n",
    "dataset = pd.concat([dataset, sex, embark, pclass], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.728622Z",
     "iopub.status.busy": "2022-02-11T18:36:16.728348Z",
     "iopub.status.idle": "2022-02-11T18:36:16.740107Z",
     "shell.execute_reply": "2022-02-11T18:36:16.739376Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.728585Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop original columns\n",
    "dataset.drop(['Sex', 'Embarked', 'Pclass'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.741784Z",
     "iopub.status.busy": "2022-02-11T18:36:16.741386Z",
     "iopub.status.idle": "2022-02-11T18:36:16.766484Z",
     "shell.execute_reply": "2022-02-11T18:36:16.765574Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.74174Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same thing using Label encoder and one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.767862Z",
     "iopub.status.busy": "2022-02-11T18:36:16.767613Z",
     "iopub.status.idle": "2022-02-11T18:36:16.77337Z",
     "shell.execute_reply": "2022-02-11T18:36:16.772629Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.767833Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.776434Z",
     "iopub.status.busy": "2022-02-11T18:36:16.776155Z",
     "iopub.status.idle": "2022-02-11T18:36:16.80336Z",
     "shell.execute_reply": "2022-02-11T18:36:16.802671Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.776401Z"
    }
   },
   "outputs": [],
   "source": [
    "# We will again import the Titanic dataset\n",
    "data = pd.read_csv('../input/titanicdataset-traincsv/train.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.804776Z",
     "iopub.status.busy": "2022-02-11T18:36:16.804542Z",
     "iopub.status.idle": "2022-02-11T18:36:16.808953Z",
     "shell.execute_reply": "2022-02-11T18:36:16.808318Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.804748Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filling the null values in embarked column with U as we've seen above\n",
    "data['Embarked'].fillna('U',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.810437Z",
     "iopub.status.busy": "2022-02-11T18:36:16.80979Z",
     "iopub.status.idle": "2022-02-11T18:36:16.820395Z",
     "shell.execute_reply": "2022-02-11T18:36:16.819516Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.810401Z"
    }
   },
   "outputs": [],
   "source": [
    "data['Sex'] = labelencoder.fit_transform(data['Sex'])\n",
    "# data['Pclass'] = labelencoder.fir_transform(data['Sex']) # we won't do this as this column is already under integer values\n",
    "data['Embarked'] = labelencoder.fit_transform(data['Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:36:16.821629Z",
     "iopub.status.busy": "2022-02-11T18:36:16.821251Z",
     "iopub.status.idle": "2022-02-11T18:36:16.847271Z",
     "shell.execute_reply": "2022-02-11T18:36:16.846307Z",
     "shell.execute_reply.started": "2022-02-11T18:36:16.821597Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note that enbarked and sex columns have been changed into numeric column\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this transformation has its own problem. In column called Embarked which has 3 categorical values ('S', 'C', 'Q','U'). Label Encoder will convert these values into 0, 1, 2 and 3. Although in the original dataset, there is no relation between 'S', 'C', 'Q' and 'U'  but after label encoding it appears that there is some kind of relation like 'U' > 'Q' > 'C' > 'S' (which is not true) as 'U' is encoded to 3, 'Q' is encoded to 2, 'C' is encoded to 1 and 'S' is encoded to 0. So, in order to remove this confusion, we need to further use one hot encoding on it to create different columns corresponding to 'S', 'C' and 'Q' which will contain only zero and ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the Label Encoded values to One Hot Encoded values\n",
    "\n",
    "One Hot Encoder takes a column which has been label encoded, and then splits the column into multiple columns. The numbers are replaced by zeros and ones, depending on which column has what value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:38:18.652412Z",
     "iopub.status.busy": "2022-02-11T18:38:18.651476Z",
     "iopub.status.idle": "2022-02-11T18:38:18.657323Z",
     "shell.execute_reply": "2022-02-11T18:38:18.656216Z",
     "shell.execute_reply.started": "2022-02-11T18:38:18.652361Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:42:49.625341Z",
     "iopub.status.busy": "2022-02-11T18:42:49.624897Z",
     "iopub.status.idle": "2022-02-11T18:42:49.635608Z",
     "shell.execute_reply": "2022-02-11T18:42:49.635073Z",
     "shell.execute_reply.started": "2022-02-11T18:42:49.625296Z"
    }
   },
   "outputs": [],
   "source": [
    "sex = pd.DataFrame(enc.fit_transform(data[['Sex']]).toarray())\n",
    "pclass = pd.DataFrame(enc.fit_transform(data[['Pclass']]).toarray())\n",
    "embarked = pd.DataFrame(enc.fit_transform(data[['Embarked']]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:43:09.487711Z",
     "iopub.status.busy": "2022-02-11T18:43:09.487394Z",
     "iopub.status.idle": "2022-02-11T18:43:09.506416Z",
     "shell.execute_reply": "2022-02-11T18:43:09.505721Z",
     "shell.execute_reply.started": "2022-02-11T18:43:09.487673Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sex)\n",
    "print(pclass)\n",
    "print(embarked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:53:32.605011Z",
     "iopub.status.busy": "2022-02-11T18:53:32.603669Z",
     "iopub.status.idle": "2022-02-11T18:53:32.652253Z",
     "shell.execute_reply": "2022-02-11T18:53:32.6514Z",
     "shell.execute_reply.started": "2022-02-11T18:53:32.604945Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.concat([data,sex,pclass,embarked],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T18:55:11.486588Z",
     "iopub.status.busy": "2022-02-11T18:55:11.486264Z",
     "iopub.status.idle": "2022-02-11T18:55:11.51394Z",
     "shell.execute_reply": "2022-02-11T18:55:11.513081Z",
     "shell.execute_reply.started": "2022-02-11T18:55:11.486558Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly not my favorite as you can see the mess above. Also, If any of you can suggest how can I add prefixes in the one hot encoding then please comment as I don't know it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove invalid values from dataset in Python\n",
    "Domain knowledge plays a crucial role in data wrangling. Sometimes, there are no missing values in the dataset but there are a lot of invalid values which we need to manually identify and remove those invalid values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, consider \"[Pima Indians Diabetes](https://www.kaggle.com/uciml/pima-indians-diabetes-database)\" dataset which predicts the onset of diabetes within 5 years in Pima Indians, given medical details. This dataset has a lot of invalid values which we will try to remove in this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T19:01:54.389258Z",
     "iopub.status.busy": "2022-02-11T19:01:54.388907Z",
     "iopub.status.idle": "2022-02-11T19:01:54.417048Z",
     "shell.execute_reply": "2022-02-11T19:01:54.41646Z",
     "shell.execute_reply.started": "2022-02-11T19:01:54.389227Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T19:02:36.539698Z",
     "iopub.status.busy": "2022-02-11T19:02:36.539116Z",
     "iopub.status.idle": "2022-02-11T19:02:36.549432Z",
     "shell.execute_reply": "2022-02-11T19:02:36.548664Z",
     "shell.execute_reply.started": "2022-02-11T19:02:36.539656Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's check null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmm, no null values that quit strange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T19:04:38.744938Z",
     "iopub.status.busy": "2022-02-11T19:04:38.744572Z",
     "iopub.status.idle": "2022-02-11T19:04:38.782341Z",
     "shell.execute_reply": "2022-02-11T19:04:38.781454Z",
     "shell.execute_reply.started": "2022-02-11T19:04:38.744894Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are minimum zero values in some columns. Here domain knowledge plays a vital role. Although there are no null or missing values in this dataset, but there are a lot of invalid values (zero) in the above column. Lets see how many zero values are there in the above columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T19:07:55.876209Z",
     "iopub.status.busy": "2022-02-11T19:07:55.875921Z",
     "iopub.status.idle": "2022-02-11T19:07:55.884206Z",
     "shell.execute_reply": "2022-02-11T19:07:55.883682Z",
     "shell.execute_reply.started": "2022-02-11T19:07:55.876176Z"
    }
   },
   "outputs": [],
   "source": [
    "print((df[['Pregnancies','Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T19:09:05.544098Z",
     "iopub.status.busy": "2022-02-11T19:09:05.543804Z",
     "iopub.status.idle": "2022-02-11T19:09:05.555229Z",
     "shell.execute_reply": "2022-02-11T19:09:05.554378Z",
     "shell.execute_reply.started": "2022-02-11T19:09:05.544061Z"
    }
   },
   "outputs": [],
   "source": [
    "#In order to handle these invalid zero values, we will mark these values as NaN. \n",
    "df[['Pregnancies','Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = df[['Pregnancies','Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0, np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-11T19:09:34.594965Z",
     "iopub.status.busy": "2022-02-11T19:09:34.594704Z",
     "iopub.status.idle": "2022-02-11T19:09:34.603375Z",
     "shell.execute_reply": "2022-02-11T19:09:34.60254Z",
     "shell.execute_reply.started": "2022-02-11T19:09:34.594936Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now print null values:\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's it for this notebook. Don't forget to upvote and comment your suggestions and feedback. Thanks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
